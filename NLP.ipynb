{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling from Clickbait Websites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project I wanted to look at the topics on clickbait websites.  I was interested in these topics since the content on these sites is designed specifically for clicks, not necessarily for quality. I chose to modularize my code within classes for this project to simplify the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport NLPProcessing\n",
    "from NLPProcessing import GetArticles\n",
    "from NLPProcessing import NLPPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created a class to handle all of the webscaping for the sites.  The class creates a dictionary and appends each article on each site to the dictionary.  I created this dictionary for the three sites that I was interested in scraping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.nickiswift.com/'\n",
    "nicki_swift_articles = GetArticles()\n",
    "nicki_swift_articles = nicki_swift_articles.create_article_dict(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.grunge.com/'\n",
    "grunge_articles = GetArticles()\n",
    "grunge_articles = grunge_articles.create_article_dict(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.giveitlove.com/'\n",
    "give_it_love_articles = GetArticles()\n",
    "give_it_love_articles = give_it_love_articles.create_article_dict(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wanted to put all of the data into a Mongo database so I created a database and inserted all of the articles from the three websites. The collection that I created was called \"junk_website_data\" because the sites were all clickbate sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydb = client[\"junk_website_data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each document, which was an article, had a website, title and text field was inserted into MongoDB. I ran this code for each websites dictionary that was created above. I chose to do this one site at a time to ensure the data was in the proper format before inserting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the site to be iterated over for each site\n",
    "for key, value in give_it_love_articles.items():\n",
    "    \n",
    "    data_dict = {}\n",
    "    data_dict['website'] = url\n",
    "    data_dict['title'] = str(key)\n",
    "    data_dict['text'] = str(value)\n",
    "    mydb.junk_website_data.insert_one(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran the code below to ensure that the collection was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['junk_website_data']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydb.list_collection_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean MongoDB Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I inserted the data into MongoDB, I inserted the data as raw html. In order to pull the text and titles out of the raw html, BeautifulSoup was used to extract the raw text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_cursor = mydb.junk_website_data.find({}, {'_id':0, 'title': 1}).limit(1)\n",
    "title_list = list(title_cursor)\n",
    "title = title_list[0]['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(title, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The real reason these contestants were kicked off The Bachelor'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can get clean titles!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "mydb = client[\"junk_website_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cursor = mydb.junk_website_data.find({}, {'_id':0, 'title': 1, 'text': 1})\n",
    "articles = list(text_cursor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models to be created:\n",
    "1. SVD\n",
    "2. NMF\n",
    "3. LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All models will be created with a count vectorizer and tf-idf vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD - Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "game, dog, movie, band, team, cat, war, rock, character, song, video, white, school, season, photo, actor, john, album, human, fire\n",
      "\n",
      "Topic  1\n",
      "dog, cat, photo, rescue, animals, save, credit, owner, pet, animal, humans, water, cub, look like, pup, human, photo credit, food, adopt, boat\n",
      "\n",
      "Topic  2\n",
      "cat, myth, snapchat, food, human, milk, image, kitty, kitten, credit, researchers, feral, space, ancient, paw, study, hunt, litter, domesticate, color\n",
      "\n",
      "Topic  3\n",
      "game, team, sport, season, bowl, super, players, coach, nba, super bowl, player, nfl, league, football, ball, field, basketball, cup, olympics, championship\n",
      "\n",
      "Topic  4\n",
      "game, band, song, winner, album, team, rock, dog, cat, songs, sport, bowl, player, super bowl, players, tour, nba, coach, outstanding, super\n"
     ]
    }
   ],
   "source": [
    "cv = NLPPipeline()\n",
    "cv.vectorize(articles)\n",
    "cv.fit(topic_model=TruncatedSVD(5))\n",
    "cv.display_topics(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD - TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "game, movie, band, actor, wed, dog, character, song, war, season, team, video, tweet, award, photo, rock, royal, grande, school, police\n",
      "\n",
      "Topic  1\n",
      "markle, meghan, royal, harry, prince, wed, prince harry, duchess, meghan markle, royal family, thomas, swift previously, swift previously report, nicki swift previously, engagement, grande, palace, middleton, kate, davidson\n",
      "\n",
      "Topic  2\n",
      "markle, royal, meghan, harry, prince, prince harry, duchess, meghan markle, thomas, royal family, queen, palace, dog, middleton, game, william, war, george, princess, sussex\n",
      "\n",
      "Topic  3\n",
      "grande, davidson, ariana, miller, pete, ariana grande, pete davidson, band, album, song, mac, mac miller, night live, pop star, saturday night live, comedian, saturday night, grande davidson, snl, saturday\n",
      "\n",
      "Topic  4\n",
      "lovato, sexual, drug, arrest, sobriety, abuse, overdose, addiction, charge, police, demi, allege, assault, band, tmz, kelly, weinstein, allegedly, rehab, trump\n"
     ]
    }
   ],
   "source": [
    "nltk_stop_words = set(stopwords.words('english'))\n",
    "tfidf = NLPPipeline(vectorizer=TfidfVectorizer(stop_words=nltk_stop_words, min_df=15, max_df=0.25, ngram_range=(1,3)))\n",
    "tfidf.vectorize(articles)\n",
    "tfidf.fit(topic_model=TruncatedSVD(5))\n",
    "tfidf.display_topics(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "game, movie, band, actor, wed, dog, character, song, war, season, team, video, tweet, award, photo, rock, royal, grande, school, police\n",
      "\n",
      "Topic  1\n",
      "markle, meghan, royal, harry, prince, wed, prince harry, duchess, meghan markle, royal family, thomas, swift previously, swift previously report, nicki swift previously, engagement, grande, palace, middleton, kate, davidson\n",
      "\n",
      "Topic  2\n",
      "markle, royal, meghan, harry, prince, prince harry, duchess, meghan markle, thomas, royal family, queen, palace, dog, middleton, game, william, war, george, princess, wed\n",
      "\n",
      "Topic  3\n",
      "grande, davidson, ariana, pete, miller, ariana grande, pete davidson, band, album, song, mac, mac miller, bieber, night live, pop star, saturday night live, comedian, baldwin, saturday night, grande davidson\n",
      "\n",
      "Topic  4\n",
      "lovato, drug, arrest, sexual, sobriety, abuse, addiction, charge, overdose, police, demi, allege, assault, tmz, band, allegedly, rehab, sentence, prison, cosby\n"
     ]
    }
   ],
   "source": [
    "nltk_stop_words = set(stopwords.words('english'))\n",
    "tfidf = NLPPipeline(vectorizer=TfidfVectorizer(stop_words=nltk_stop_words, min_df=15, max_df=0.25, ngram_range=(1,3)))\n",
    "tfidf.vectorize(articles)\n",
    "tfidf.fit(topic_model=TruncatedSVD(5))\n",
    "tfidf.display_topics(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF - Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "movie, war, character, president, white, actor, trump, school, men, build, job, fire, human, role, america, movies, party, stuff, murder, allegedly\n",
      "\n",
      "Topic  1\n",
      "dog, photo, rescue, save, look like, pup, cub, animals, adopt, credit, boat, animal, owner, water, photo credit, breed, pant, khan, pet, taco\n",
      "\n",
      "Topic  2\n",
      "cat, credit, food, myth, human, snapchat, image, pet, animals, owner, humans, paw, eat, milk, kitten, animal, water, kitty, sleep, fish\n",
      "\n",
      "Topic  3\n",
      "game, team, sport, season, super, bowl, players, player, coach, nba, super bowl, league, nfl, football, ball, field, score, basketball, title, cup\n",
      "\n",
      "Topic  4\n",
      "band, song, rock, album, winner, songs, roll, tour, group, stone, sing, video, john, mercury, roll stone, single, lyric, queen, albums, award\n"
     ]
    }
   ],
   "source": [
    "cv.fit(topic_model=NMF(5))\n",
    "cv.display_topics(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF - TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "game, movie, band, dog, war, character, team, song, rock, season, school, president, sport, actor, trump, white, album, men, human, murder\n",
      "\n",
      "Topic  1\n",
      "markle, meghan, royal, harry, prince, prince harry, wed, duchess, meghan markle, thomas, royal family, palace, middleton, queen, sussex, windsor, kate, princess, duchess sussex, harry meghan\n",
      "\n",
      "Topic  2\n",
      "kardashian, jenner, welcome, first child, caption, swift previously, swift previously report, nicki swift previously, mom, divorce, child together, marriage, thompson, excite, june, us weekly, girl, wed, social media, weekly\n",
      "\n",
      "Topic  3\n",
      "grande, davidson, ariana, pete, miller, ariana grande, pete davidson, engagement, comedian, night live, pop star, saturday, saturday night live, bieber, mac miller, saturday night, mac, baldwin, grande davidson, snl\n",
      "\n",
      "Topic  4\n",
      "lovato, sobriety, overdose, demi, addiction, drug, rehab, abuse, sober, health, demi lovato, substance abuse, substance, relapse, struggle, help privacy, help privacy policy, recovery, please call substance, call substance\n"
     ]
    }
   ],
   "source": [
    "tfidf = NLPPipeline(vectorizer=TfidfVectorizer(stop_words=nltk_stop_words, min_df=15, max_df=0.25, ngram_range=(1,3)))\n",
    "tfidf.vectorize(articles)\n",
    "tfidf.fit(topic_model=NMF(5))\n",
    "tfidf.display_topics(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA - Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_word = cv.vectorized_corpus.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>3084</th>\n",
       "      <th>3085</th>\n",
       "      <th>3086</th>\n",
       "      <th>3087</th>\n",
       "      <th>3088</th>\n",
       "      <th>3089</th>\n",
       "      <th>3090</th>\n",
       "      <th>3091</th>\n",
       "      <th>3092</th>\n",
       "      <th>3093</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aaron</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aback</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abandon</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbey</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbott</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3094 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0     1     2     3     4     5     6     7     8     9     ...  \\\n",
       "aaron       0     0     0     0     0     0     0     0     0     0  ...   \n",
       "aback       0     0     0     0     0     0     0     0     0     0  ...   \n",
       "abandon     0     0     0     0     0     0     0     0     0     0  ...   \n",
       "abbey       0     0     0     0     0     0     0     0     0     0  ...   \n",
       "abbott      0     0     0     0     0     0     0     0     0     0  ...   \n",
       "\n",
       "         3084  3085  3086  3087  3088  3089  3090  3091  3092  3093  \n",
       "aaron       0     0     4     0     0     0     0     0     0     0  \n",
       "aback       0     0     0     0     0     0     0     0     0     0  \n",
       "abandon     0     0     0     0     0     0     0     0     0     1  \n",
       "abbey       0     0     0     0     0     0     0     0     0     0  \n",
       "abbott      0     0     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[5 rows x 3094 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(doc_word.toarray(), cv.vectorizer.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse matrix of counts to a gensim corpus\n",
    "corpus = matutils.Sparse2Corpus(doc_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = dict((v, k) for k, v in cv.vectorizer.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lda model (equivalent to \"fit\" in sklearn)\n",
    "lda = models.LdaModel(corpus=corpus, num_topics=3, id2word=id2word, passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.005*\"movie\" + 0.004*\"character\" + 0.003*\"actor\" + 0.002*\"role\" + 0.002*\"movies\" + 0.002*\"cast\" + 0.002*\"host\" + 0.002*\"tweet\" + 0.002*\"season\" + 0.001*\"comedy\"'),\n",
       " (1,\n",
       "  '0.006*\"band\" + 0.005*\"dog\" + 0.005*\"song\" + 0.004*\"rock\" + 0.004*\"album\" + 0.003*\"photo\" + 0.002*\"songs\" + 0.002*\"roll\" + 0.002*\"credit\" + 0.002*\"tour\"'),\n",
       " (2,\n",
       "  '0.003*\"game\" + 0.002*\"war\" + 0.002*\"team\" + 0.001*\"human\" + 0.001*\"dog\" + 0.001*\"cat\" + 0.001*\"build\" + 0.001*\"water\" + 0.001*\"eat\" + 0.001*\"white\"')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.interfaces.TransformedCorpus at 0x1a38e6deb8>"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the docs from the word space to the topic space (like \"transform\" in sklearn)\n",
    "lda_corpus = lda[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the documents' topic vectors in a list so we can take a peak\n",
    "lda_docs = [doc for doc in lda_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0.37225583), (2, 0.62675345)],\n",
       " [(0, 0.2294572), (1, 0.2791612), (2, 0.4913816)],\n",
       " [(0, 0.7855218), (2, 0.21371226)],\n",
       " [(2, 0.99563247)],\n",
       " [(0, 0.3221556), (2, 0.67626154)]]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_docs[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA - TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_stop_words = set(stopwords.words('english'))\n",
    "tfidf_lda = NLPPipeline(vectorizer=TfidfVectorizer(stop_words=nltk_stop_words, min_df=15, max_df=0.25, ngram_range=(1,3)))\n",
    "tfidf_lda.vectorize(articles)\n",
    "doc_word_tfidf = tfidf_lda.vectorized_corpus.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse matrix of counts to a gensim corpus\n",
    "corpus = matutils.Sparse2Corpus(doc_word_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = dict((v, k) for k, v in tfidf_lda.vectorizer.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lda model (equivalent to \"fit\" in sklearn)\n",
    "lda = models.LdaModel(corpus=corpus, num_topics=5, id2word=id2word, passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.001*\"mcphee\" + 0.000*\"grande\" + 0.000*\"jolie\" + 0.000*\"davidson\" + 0.000*\"pitt\" + 0.000*\"lowell\" + 0.000*\"katharine mcphee\" + 0.000*\"david foster\" + 0.000*\"capri\" + 0.000*\"carpet debut\"'),\n",
       " (1,\n",
       "  '0.000*\"thanos\" + 0.000*\"spider\" + 0.000*\"spider man\" + 0.000*\"batman\" + 0.000*\"comics\" + 0.000*\"spacey\" + 0.000*\"marvel\" + 0.000*\"joker\" + 0.000*\"movie\" + 0.000*\"character\"'),\n",
       " (2,\n",
       "  '0.002*\"lovato\" + 0.001*\"sobriety\" + 0.001*\"hyland\" + 0.000*\"demi\" + 0.000*\"overdose\" + 0.000*\"help privacy\" + 0.000*\"help privacy policy\" + 0.000*\"substance abuse mental\" + 0.000*\"abuse mental health\" + 0.000*\"abuse mental\"'),\n",
       " (3,\n",
       "  '0.001*\"game\" + 0.001*\"dog\" + 0.001*\"band\" + 0.001*\"movie\" + 0.001*\"war\" + 0.001*\"team\" + 0.001*\"character\" + 0.001*\"song\" + 0.001*\"photo\" + 0.001*\"rock\"'),\n",
       " (4,\n",
       "  '0.001*\"chopra\" + 0.001*\"jonas\" + 0.001*\"child together baby\" + 0.001*\"together baby\" + 0.001*\"child together\" + 0.000*\"excite baby\" + 0.000*\"first child\" + 0.000*\"baby news\" + 0.000*\"share excite\" + 0.000*\"baby news privacy\"')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
