{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Langauge Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Topic Modeling of Articles\n",
    "2. Clustering of sensational websites with news outlets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import os\n",
    "import time\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pymongo import MongoClient\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "import re # Regular expression library\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "\n",
    "\n",
    "chromedriver = \"/Applications/chromedriver\" # path to the chromedriver executable\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = 'https://www.nickiswift.com/'\n",
    "#url = 'https://www.grunge.com/'\n",
    "url = 'http://www.giveitlove.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(chromedriver)\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll():\n",
    "   \n",
    "    '''Scrolls to the bottom of a long webpage for a max of 30 seconds'''\n",
    "    SCROLL_PAUSE_TIME = 4\n",
    "\n",
    "    # Get scroll height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    t_end = time.time() + 120 \n",
    "    \n",
    "    while time.time() < t_end:\n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Wait to load page\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_article_list(soup):\n",
    "    ''' Creates an article list from the home page of a website'''\n",
    "    \n",
    "    article_list = []\n",
    "    for link in soup.find_all('a'): \n",
    "        try:\n",
    "            if url in link['href']:\n",
    "                article_list.append(link['href'])\n",
    "        except:\n",
    "            break\n",
    "    return article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_article_dict(article_list):\n",
    "    '''returns the article text for each article in a list of articles'''\n",
    "    article_dict = {}\n",
    "    \n",
    "    for article in article_list:\n",
    "        \n",
    "        response = requests.get(article)\n",
    "        page = response.text\n",
    "        soup = BeautifulSoup(page, \"lxml\")\n",
    "        article_title = soup.find('h1')\n",
    "        article_text = soup.find_all(['h2', 'p'])\n",
    "        \n",
    "        if len(article_text) > 3:\n",
    "                   \n",
    "            article_dict[article_title] = article_text\n",
    "    \n",
    "    return article_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_dict = create_article_dict(article_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_article_dictionary_list(url):\n",
    "    \n",
    "    #scroll()\n",
    "    soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "    article_list = create_article_list(soup)\n",
    "    \n",
    "    return create_article_dict(article_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = create_article_dictionary_list(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "grunge_data = create_article_dictionary_list(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "giveitlove_data = create_article_dictionary_list(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydb = client[\"junk_website_data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert into MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'giveitlove_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4e679370b521>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgiveitlove_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdata_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'website'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'giveitlove_data' is not defined"
     ]
    }
   ],
   "source": [
    "for key, value in giveitlove_data.items():\n",
    "    \n",
    "    data_dict = {}\n",
    "    data_dict['website'] = url\n",
    "    data_dict['title'] = str(key)\n",
    "    data_dict['text'] = str(value)\n",
    "    mydb.junk_website_data.insert_one(data_dict)\n",
    "    \n",
    "#     print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['admin', 'config', 'junk_website_data', 'local']\n"
     ]
    }
   ],
   "source": [
    "print(client.list_database_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['junk_website_data']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydb.list_collection_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean MongoDB Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_cursor = mydb.junk_website_data.find({}, {'_id':0, 'title': 1}).limit(1)\n",
    "title_list = list(title_cursor)\n",
    "title = title_list[0]['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(title, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The real reason these contestants were kicked off The Bachelor'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "mydb = client[\"junk_website_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cursor = mydb.junk_website_data.find({}, {'_id':0, 'title': 1, 'text': 1})\n",
    "articles = list(text_cursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLP_Pipeline:\n",
    "    \n",
    "    def __init__(self, vectorizer=None):\n",
    "\n",
    "        self.nltk_stop_words = set(stopwords.words('english'))\n",
    "        if not vectorizer:\n",
    "            vectorizer = CountVectorizer(stop_words=self.nltk_stop_words, min_df=15, max_df=0.25, ngram_range=(1,3))\n",
    "        self.model = None\n",
    "        self.vectorizer = vectorizer\n",
    "        \n",
    "    def remove_html(self, article):\n",
    "\n",
    "        text_corpus_list = []\n",
    "\n",
    "        text = article['text']\n",
    "        soup = BeautifulSoup(text, \"lxml\")\n",
    "        article_text = soup.get_text()[1:-1]\n",
    "\n",
    "        return article_text\n",
    "    \n",
    "    def text_cleaning(self, article):\n",
    "    \n",
    "        clean_text = re.sub('[%s]' % re.escape(string.punctuation), ' ', article)\n",
    "        clean_text = re.sub('\\w*\\d\\w*', ' ', clean_text)\n",
    "        clean_text = clean_text.lower() \n",
    "\n",
    "        return clean_text\n",
    "\n",
    "    def text_lemmatizing(self, article):\n",
    "    \n",
    "        lemmatized_word_list = []\n",
    "        words = word_tokenize(article)\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        for word in words:\n",
    "            lemmatized_word = wordnet_lemmatizer.lemmatize(word, pos='v')\n",
    "            lemmatized_word_list.append(lemmatized_word)\n",
    "\n",
    "        lemmatized_word_string = ' '.join(lemmatized_word_list)\n",
    "        \n",
    "        return lemmatized_word_string\n",
    "        \n",
    "    def fit(self, articles):\n",
    "        \n",
    "        cleaned_article_corpus = []\n",
    "        for article in articles:\n",
    "    \n",
    "            article_no_html = self.remove_html(article)\n",
    "            clean_article = self.text_cleaning(article_no_html)\n",
    "            lemmatized_article = self.text_lemmatizing(clean_article)\n",
    "            cleaned_article_corpus.append(lemmatized_article)\n",
    "        \n",
    "        self.model = self.vectorizer.fit_transform(cleaned_article_corpus)\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD - Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3094x20521 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1264814 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = NLP_Pipeline()\n",
    "cv.fit(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "game, dog, movie, band, team, cat, war, rock, character, song, video, white, school, season, photo, actor, john, album, human, fire\n",
      "\n",
      "Topic  1\n",
      "dog, cat, photo, rescue, animals, save, credit, owner, pet, animal, humans, water, look like, cub, pup, human, photo credit, food, adopt, boat\n",
      "\n",
      "Topic  2\n",
      "cat, myth, snapchat, food, human, milk, image, kitty, kitten, credit, researchers, space, feral, ancient, paw, study, hunt, litter, domesticate, color\n",
      "\n",
      "Topic  3\n",
      "game, team, sport, season, bowl, super, players, coach, nba, super bowl, player, nfl, league, football, ball, field, basketball, cup, olympics, championship\n",
      "\n",
      "Topic  4\n",
      "game, band, song, winner, album, team, rock, dog, cat, songs, sport, bowl, player, super bowl, players, tour, nba, coach, outstanding, super\n"
     ]
    }
   ],
   "source": [
    "lsa = TruncatedSVD(5)\n",
    "doc_topic = lsa.fit_transform(cv.model)\n",
    "display_topics(lsa, cv.vectorizer.get_feature_names(), 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD - TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words=nltk_stop_words, min_df=15, max_df=0.25, ngram_range=(1,3))\n",
    "tfidf = NLP_Pipeline(vectorizer = tfidf_vectorizer)\n",
    "tfidf.fit(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "game, movie, band, actor, wed, dog, character, song, war, season, team, video, tweet, award, photo, rock, royal, grande, school, police\n",
      "\n",
      "Topic  1\n",
      "markle, meghan, royal, harry, prince, wed, prince harry, duchess, meghan markle, royal family, thomas, swift previously report, swift previously, nicki swift previously, grande, engagement, palace, middleton, kate, davidson\n",
      "\n",
      "Topic  2\n",
      "markle, royal, meghan, harry, prince, prince harry, duchess, meghan markle, thomas, royal family, queen, palace, dog, middleton, game, william, war, george, princess, sussex\n",
      "\n",
      "Topic  3\n",
      "grande, davidson, ariana, pete, miller, ariana grande, pete davidson, band, album, song, mac, mac miller, night live, pop star, comedian, saturday night live, bieber, saturday night, baldwin, grande davidson\n",
      "\n",
      "Topic  4\n",
      "lovato, sexual, drug, sobriety, arrest, abuse, band, addiction, overdose, charge, demi, assault, allege, police, weinstein, tmz, argento, rehab, accuse, allegedly\n"
     ]
    }
   ],
   "source": [
    "lsa = TruncatedSVD(5)\n",
    "doc_topic = lsa.fit_transform(tfidf.model)\n",
    "display_topics(lsa, tfidf.vectorizer.get_feature_names(), 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF - Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "movie, war, character, president, white, actor, trump, school, men, build, job, fire, human, role, america, movies, party, stuff, murder, allegedly\n",
      "\n",
      "Topic  1\n",
      "dog, photo, rescue, save, look like, pup, cub, animals, adopt, credit, boat, animal, owner, water, photo credit, breed, pant, khan, pet, taco\n",
      "\n",
      "Topic  2\n",
      "cat, credit, food, myth, human, snapchat, image, pet, animals, owner, humans, paw, eat, milk, kitten, animal, water, kitty, sleep, fish\n",
      "\n",
      "Topic  3\n",
      "game, team, sport, season, super, bowl, players, player, coach, nba, super bowl, league, nfl, football, ball, field, score, basketball, title, cup\n",
      "\n",
      "Topic  4\n",
      "band, song, rock, album, winner, songs, roll, tour, group, stone, sing, video, john, mercury, roll stone, single, lyric, queen, albums, award\n"
     ]
    }
   ],
   "source": [
    "#Count Vectorizer\n",
    "nmf_model = NMF(5)\n",
    "doc_topic = nmf_model.fit_transform(cv.model)\n",
    "display_topics(nmf_model, cv.vectorizer.get_feature_names(), 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF - TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "game, movie, band, dog, war, character, team, song, rock, season, school, president, sport, actor, trump, white, album, men, human, murder\n",
      "\n",
      "Topic  1\n",
      "markle, meghan, royal, harry, prince, prince harry, wed, duchess, meghan markle, thomas, royal family, palace, middleton, queen, sussex, windsor, kate, princess, duchess sussex, harry meghan\n",
      "\n",
      "Topic  2\n",
      "kardashian, jenner, welcome, first child, caption, swift previously, swift previously report, nicki swift previously, mom, divorce, child together, marriage, thompson, excite, june, us weekly, girl, wed, social media, weekly\n",
      "\n",
      "Topic  3\n",
      "grande, davidson, ariana, pete, miller, ariana grande, pete davidson, engagement, comedian, night live, pop star, saturday, saturday night live, bieber, mac miller, saturday night, mac, baldwin, grande davidson, snl\n",
      "\n",
      "Topic  4\n",
      "lovato, sobriety, overdose, demi, addiction, drug, rehab, abuse, sober, health, demi lovato, substance abuse, substance, relapse, struggle, help privacy, help privacy policy, recovery, please call substance, call substance\n"
     ]
    }
   ],
   "source": [
    "#Using TF-IDF\n",
    "nmf_model = NMF(5)\n",
    "doc_topic = nmf_model.fit_transform(tfidf.model)\n",
    "display_topics(nmf_model, tfidf.vectorizer.get_feature_names(), 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA - Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_word = count_vectorizer.transform(cleaned_article_corpus).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>3084</th>\n",
       "      <th>3085</th>\n",
       "      <th>3086</th>\n",
       "      <th>3087</th>\n",
       "      <th>3088</th>\n",
       "      <th>3089</th>\n",
       "      <th>3090</th>\n",
       "      <th>3091</th>\n",
       "      <th>3092</th>\n",
       "      <th>3093</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aaron</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aback</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abandon</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbey</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbott</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3094 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0     1     2     3     4     5     6     7     8     9     ...  \\\n",
       "aaron       0     0     0     0     0     0     0     0     0     0  ...   \n",
       "aback       0     0     0     0     0     0     0     0     0     0  ...   \n",
       "abandon     0     0     0     0     0     0     0     0     0     0  ...   \n",
       "abbey       0     0     0     0     0     0     0     0     0     0  ...   \n",
       "abbott      0     0     0     0     0     0     0     0     0     0  ...   \n",
       "\n",
       "         3084  3085  3086  3087  3088  3089  3090  3091  3092  3093  \n",
       "aaron       0     0     4     0     0     0     0     0     0     0  \n",
       "aback       0     0     0     0     0     0     0     0     0     0  \n",
       "abandon     0     0     0     0     0     0     0     0     0     1  \n",
       "abbey       0     0     0     0     0     0     0     0     0     0  \n",
       "abbott      0     0     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[5 rows x 3094 columns]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(doc_word.toarray(), count_vectorizer.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse matrix of counts to a gensim corpus\n",
    "corpus = matutils.Sparse2Corpus(doc_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = dict((v, k) for k, v in count_vectorizer.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lda model (equivalent to \"fit\" in sklearn)\n",
    "lda = models.LdaModel(corpus=corpus, num_topics=3, id2word=id2word, passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.003*\"band\" + 0.002*\"trump\" + 0.002*\"movies\" + 0.002*\"murder\" + 0.001*\"arrest\" + 0.001*\"director\" + 0.001*\"album\" + 0.001*\"mercury\" + 0.001*\"comedy\" + 0.001*\"roll stone\"'),\n",
       " (1,\n",
       "  '0.004*\"dog\" + 0.002*\"cat\" + 0.002*\"band\" + 0.002*\"credit\" + 0.002*\"sport\" + 0.001*\"image\" + 0.001*\"animals\" + 0.001*\"century\" + 0.001*\"animal\" + 0.001*\"space\"'),\n",
       " (2,\n",
       "  '0.002*\"divorce\" + 0.002*\"winner\" + 0.002*\"harry\" + 0.002*\"tmz\" + 0.002*\"grande\" + 0.001*\"royal\" + 0.001*\"rapper\" + 0.001*\"kardashian\" + 0.001*\"meghan\" + 0.001*\"prince\"')]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.interfaces.TransformedCorpus at 0x1a38e6deb8>"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the docs from the word space to the topic space (like \"transform\" in sklearn)\n",
    "lda_corpus = lda[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the documents' topic vectors in a list so we can take a peak\n",
    "lda_docs = [doc for doc in lda_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0.37225583), (2, 0.62675345)],\n",
       " [(0, 0.2294572), (1, 0.2791612), (2, 0.4913816)],\n",
       " [(0, 0.7855218), (2, 0.21371226)],\n",
       " [(2, 0.99563247)],\n",
       " [(0, 0.3221556), (2, 0.67626154)]]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_docs[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA -TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words=nltk_stop_words, min_df=15, max_df=0.25, ngram_range=(1,3))\n",
    "doc_word_tfidf = tfidf_vectorizer.fit_transform(cleaned_article_corpus).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse matrix of counts to a gensim corpus\n",
    "corpus = matutils.Sparse2Corpus(doc_word_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = dict((v, k) for k, v in tfidf_vectorizer.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lda model (equivalent to \"fit\" in sklearn)\n",
    "lda = models.LdaModel(corpus=corpus, num_topics=5, id2word=id2word, passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.001*\"game\" + 0.001*\"dog\" + 0.001*\"band\" + 0.001*\"movie\" + 0.001*\"war\" + 0.001*\"team\" + 0.001*\"character\" + 0.001*\"song\" + 0.001*\"photo\" + 0.001*\"rock\"'),\n",
       " (1,\n",
       "  '0.000*\"moonves\" + 0.000*\"frankel\" + 0.000*\"shield\" + 0.000*\"mercury\" + 0.000*\"chen\" + 0.000*\"farrow\" + 0.000*\"sexual\" + 0.000*\"freddie\" + 0.000*\"brolin\" + 0.000*\"freddie mercury\"'),\n",
       " (2,\n",
       "  '0.002*\"grande\" + 0.002*\"davidson\" + 0.001*\"kardashian\" + 0.001*\"engagement\" + 0.001*\"thompson\" + 0.001*\"khloé\" + 0.001*\"swift previously report\" + 0.001*\"swift previously\" + 0.001*\"nicki swift previously\" + 0.001*\"pete davidson\"'),\n",
       " (3,\n",
       "  '0.001*\"sorrentino\" + 0.001*\"child together\" + 0.001*\"chopra\" + 0.001*\"jonas\" + 0.001*\"wilkinson\" + 0.001*\"first child\" + 0.001*\"baby news\" + 0.001*\"excite baby\" + 0.001*\"second child\" + 0.001*\"kendra\"'),\n",
       " (4,\n",
       "  '0.003*\"markle\" + 0.002*\"meghan\" + 0.002*\"lovato\" + 0.002*\"royal\" + 0.001*\"harry\" + 0.001*\"prince harry\" + 0.001*\"duchess\" + 0.001*\"prince\" + 0.001*\"meghan markle\" + 0.001*\"sobriety\"')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
